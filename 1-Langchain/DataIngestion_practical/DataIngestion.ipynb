{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data or document Loaders:\n",
    "\n",
    "we have different ways to load documents. We can use the following loaders:\n",
    " \n",
    " - Wikipedia loader\n",
    " - Text loader\n",
    " - PDF loader\n",
    " - Web based loader\n",
    " - Arxiv loader\n",
    " - Google Scholar loader\n",
    " - Semantic Scholar loader\n",
    " - DOI loader etc. we can find all the loaders in langchain_community.document_loaders documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text loader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "TextLoader = TextLoader('speech.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'speech.txt'}, page_content='In the 2019 National and State election held in April and May 2019, YSR Congress Party swept the polls and won 151 of the total 175 assembly seats and 22 of the 25 Lok Sabha seats in Andhra Pradesh. He took oath as the Chief Minister on 30 May 2019.[52] His chief ministership has been marked by a slew of welfare schemes such as Jagananna Amma Vodi, Navaratnalu.[53] Jagananna Amma Vodi provides financial assistance for mothers or guardians from the Below Poverty Line, to educate their children.[54][55] Navaratnalu is a collection of nine welfare schemes covering farmers, women, medical and health, education and Special Category Status.[53] He scrapped the plans for a new capital at Amaravati, proposed by the former TDP government, and has proposed three different capitals for the judicial, administrative and legislative branches at Kurnool, Amaravati and Visakhapatnam respectively.[56] The proposal resulted in widespread protests by the farmers of Amaravati.[57] The Andhra Pradesh High Court in a March 2022 ruling directed the Government of Andhra Pradesh to continue developing Amaravati and adjudicated that the government \"lacked the competence to make any legislation for shifting, bifurcating or trifurcating the capital\".[58]\\n\\nAs of April 2023, it was reported by the Association for Democratic Reforms that he was the richest Chief Minister in India, with total assets of 510 crore')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents  = TextLoader.load()\n",
    "\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the 2019 National and State election held in April and May 2019, YSR Congress Party swept the polls and won 151 of the total 175 assembly seats and 22 of the 25 Lok Sabha seats in Andhra Pradesh. He took oath as the Chief Minister on 30 May 2019.[52] His chief ministership has been marked by a slew of welfare schemes such as Jagananna Amma Vodi, Navaratnalu.[53] Jagananna Amma Vodi provides financial assistance for mothers or guardians from the Below Poverty Line, to educate their children.[54][55] Navaratnalu is a collection of nine welfare schemes covering farmers, women, medical and health, education and Special Category Status.[53] He scrapped the plans for a new capital at Amaravati, proposed by the former TDP government, and has proposed three different capitals for the judicial, administrative and legislative branches at Kurnool, Amaravati and Visakhapatnam respectively.[56] The proposal resulted in widespread protests by the farmers of Amaravati.[57] The Andhra Pradesh High Court in a March 2022 ruling directed the Government of Andhra Pradesh to continue developing Amaravati and adjudicated that the government \"lacked the competence to make any legislation for shifting, bifurcating or trifurcating the capital\".[58]\\n\\nAs of April 2023, it was reported by the Association for Democratic Reforms that he was the richest Chief Minister in India, with total assets of 510 crore'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'assignement2.pdf', 'page': 0}, page_content='Task 1: Data Privacy and Security Analysis  \\nI. Introduction  \\nImportance of Data Privacy and Security : In the digital age, vast amounts of sensitive data are \\ncollected and processed, making data privacy and security paramount. Organizations have an \\nethical and legal obligation to protect this information.  \\nPrevalence of Data Breaches : Despite increased awareness, data breaches remain a persistent \\nthreat, highlighting the need for robust security measures and ethical data handling practices.  \\nFocus of the Essay:  This essay analyzes the recent MOVEit Transfer data breach, examining its \\ncauses, ethical implications, and potential solutions to prevent similar incidents.  \\n \\nII. Background  \\nData Privacy:  Focuses on individual rights to control personal information. Key principles \\ninclude:  \\nConsent:  Individuals should be informed about and agree to data collection and use.  \\nTransparency:  Organizations should be open about their data practices.  \\nPurpose Limitation:  Data should only be used for the purposes for which it was collected.  \\nData Minimization:  Only necessary data should be collected and processed.  \\nData Security:  Involves technical and organizational measures to protect data, including:  \\nEncryption:  Encoding data to prevent unauthorized access.  \\nAccess Controls:  Limiting who can access sensitive information.  \\nVulnerability Management:  Identifying and addressing security weaknesses.  \\nSecurity Audits:  Regularly assessing security practices.  \\n \\nIII. The MOVEit Transfer Data Breach  \\nOverview:  In May 2023, a critical vulnerability in MOVEit Transfer, a managed file transfer \\nsoftware, was exploited, allowing unauthorized access to sensitive data.  \\nImpact:  The breach affected numerous organizations globally, including government agencies, \\nfinancial institutions, healthcare providers, and technology companies.  \\nTypes of Data Compromised:  Potentially included names, addresses, social security numbers, \\nfinancial details, and other personal information.  '),\n",
       " Document(metadata={'source': 'assignement2.pdf', 'page': 1}, page_content=' \\nIV . Ethical Implications  \\nLack of Adequate Security:  The vulnerability exploited highlights a failure to implement \\nsufficient security measures, violating the ethical responsibility to protect sensitive data.  \\nLack of Transparency:  Delays in notifying affected individuals demonstrate a lack of \\ntransparency, hindering their ability to take protective measures.  \\nPotential Misuse of Data:  Compromised data could be used for identity theft, financial fraud, or \\nother malicious activities, causing significant harm to individuals.  \\n \\nV . Solutions and Recommendations  \\n1. Strengthening Security Measures:  \\no Strong Encryption: Use robust encryption algorithms like AES -256 for data at rest and \\nTLS/SSL for data in transit. Securely manage encryption keys.  \\no Regular Updates and Patching: Establish a process for promptly identifying, testing, and \\ndeploying software updates and patches. Utilize automated vulnerability scanning tools.  \\no Penetration Testing and Vulnerability Assessments: Conduct regular penetration testing \\nand vulnerability assessments to proactively identify and address security risks.  \\no Multi -Factor Authentication and Access Controls: Implement multi -factor authentication \\nand access controls, such as role -based access control (RBAC) and least privilege \\nprinciples, to limit unauthorized access.  \\n2.Improving Data Governance:  \\no Data Privacy Policies and Procedures: Establish clear data privacy policies and \\nprocedures that comply with relevant regulations (e.g., GDPR, CCPA).  \\no Data Privacy Impact Assessments: Conduct regular data privacy impact assessments to \\nidentify and mitigate privacy risks.  \\no Data Protection Officer: Appoint a data protection officer to oversee data privacy and \\nsecurity practices.  \\no Employee Training: Provide regular training to employees on data privacy and security \\nbest practices, emphasizing their role in protecting sensitive information.  \\no Enhancing Incident Response:  \\no Incident Response Plan: Develop a comprehensive incident response plan that outlines \\nprocedures for identifying, containing, and recovering from data breaches.  \\no Regular Drills: Conduct regular drills to test the effectiveness of the incident response \\nplan and ensure preparedness.  \\n \\n '),\n",
       " Document(metadata={'source': 'assignement2.pdf', 'page': 2}, page_content='VI. Conclusion  \\nLessons Learned : The MOVEit Transfer data breach underscores the critical importance of data \\nprivacy and security in data science.  \\nEthical Imperative:  Organizations must prioritize the protection of sensitive information and \\nuphold ethical data handling practices to maintain trust and prevent harm.  \\nProactive Measures:  By implementing strong security measures, improving data governance, \\nand enhancing incident response capabilities, organizations can mitigate the risk of data breaches \\nand safeguard sensitive information.  \\n \\nSources:  \\nWikipedia:  https://en.wikipedia.org/wiki/2023_MOVEit_data_breach  \\nNCSC (UK National Cyber Security Centre): https://www.ncsc.gov.uk/information/moveit -\\nvulnerability  \\nExperian : https://www.experian.com/blogs/ask -experian/moveit -data-breach/  \\nIT Governance USA: https://www.itgovernanceusa.com/blog/moveit -breach -over-1000 -\\norganizations -and-60-million -individuals -affected  \\nORX News: https://orx.org/resource/moveit -transfer -data-breaches  \\nNational Institute of Standards and Technology (NIST): https://www.nist.gov/cyberframework  \\nInternational Association of Privacy Professionals (IAPP): https://iapp.org/  \\n \\n \\nTask2: Bias in Data Collection and Algorithms  \\nIntroduction:  Data science strives for accurate and unbiased predictions. However, biases \\nin data and algorithms can hinder this goal. This report examines the \"California Housing Prices\" \\ndataset and the Linear Regression algorithm to identify potential biases and their  implications. \\nBy analyzing these biases, we aim to highlight the importance of ethical considerations and \\npropose mitigation strategies for fair predictive modeling.  \\nDataset and Algorithm Overview  \\nDataset: California Housing Prices (https://www.kaggle.com/datasets/camnugent/california -\\nhousing -prices)  '),\n",
       " Document(metadata={'source': 'assignement2.pdf', 'page': 3}, page_content=\"This dataset contains information about housing prices in California districts derived from the \\n1990 census. It includes features like median income, housing median age, total rooms, total \\nbedrooms, population, households, latitude, and longitude.  \\nAlgorithm: Linear Regression  \\nWe'll use this  algorithm  to predict median house value based on the other features in the dataset.  \\nBias Analysis  \\n1. Identify  Potential Biases in the Dataset  \\no Data Collection Bias: The data is from the 1990 census, which may not accurately \\nreflect the current housing market in California. This could lead to outdated or \\ninaccurate predictions.  \\no Representation Bias: The dataset may not be fully representative of all communities \\nin California. Certain areas or demographics might be over - or under -represented, \\nleading to biased predictions for those groups.  \\no Historical Bias: Historical factors like redlining or discriminatory housing practices \\ncould be embedded in the data, potentially leading to biased predictions that \\nperpetuate existing inequalities.  \\n2. Identif y Potential Biases in the Algorithm  \\no Algorithmic Bias: Linear Regression can amplify biases present in the data. If certain \\nfeatures are correlated with protected attributes (e.g., race or income), the algorithm \\nmight inadvertently perpetuate those biases in its predictions.  \\no Model Selection Bias: Linear Regression assumes a linear relationship between \\nfeatures and the target variable. If this assumption is violated, the model might \\nproduce biased predictions.  \\nImplications of Bias  \\nUnfair or discriminatory outcomes:  Biased predictions could lead to unfair or discriminatory \\noutcomes in the housing market. For example, the model might overestimate housing values in \\npredominantly white neighborhoods and underestimate values in predominantly minority \\nneighborhoods.  \\nPerpetuation of inequality:  If the model reinforces existing biases, it could contribute to the \\nperpetuation of housing inequality and discrimination.  \\nErosion of trust:  Biased predictions can erode trust in the model and its use in housing -related \\ndecisions  \\nMitigation Strategies  \\n1. Data -level Mitigation:  \"),\n",
       " Document(metadata={'source': 'assignement2.pdf', 'page': 4}, page_content='o Data Collection: Supplement the dataset with more recent data to address the data \\ncollection bias.  \\no Data Preprocessing: Analyze the data for representation bias and apply techniques \\nlike re -sampling or re -weighting to address imbalances.  \\no Feature Engineering: Create new features that might help mitigate bias, such as \\nindicators of neighborhood diversity or historical disadvantage.  \\n2. Algorithm -level Mitigation : \\no Regularization: Use regularization techniques like L1 or L2 regularization to prevent \\noverfitting to biased data.  \\no Fairness -aware Algorithms: Explore fairness -aware machine learning techniques that \\ncan be applied to linear regression to mitigate bias.  \\nConclusion  \\nThis report identified potential biases in the \"California Housing Prices\" dataset and Linear \\nRegression. Unaddressed, these biases could perpetuate inequalities. Mitigating these biases is \\ncrucial for ethical and fair predictive modeling in housing market s. \\n \\n \\nTask 3: Policy for Transparency and Accountability in Data Science \\nOperations  \\n1. Introduction  \\nXYZ company  is committed to conducting its data science operations with the highest standards \\nof transparency and accountability. This policy outlines the procedures and mechanisms we \\nemploy to ensure responsible and ethical use of data and algorithms.  \\n2. Algorithm Audits  \\n• Purpose:  To assess the fairness, accuracy, and potential biases of our algorithms.  \\n• Frequency:  Audits will be conducted:  \\no Prior to deployment of any new algorithm.  \\no Annually for existing algorithms.  \\no Whenever significant changes are made to an algorithm or its training data.  \\n• Procedure:   \\no A diverse team of data scientists, ethicists, and domain experts will conduct the \\naudits.  \\no Audits will include:  '),\n",
       " Document(metadata={'source': 'assignement2.pdf', 'page': 5}, page_content=\"▪ Review of the algorithm's design and implementation.  \\n▪ Analysis of the training data for biases and representativeness.  \\n▪ Evaluation of the algorithm's performance across different demographic \\ngroups.  \\n▪ Assessment of the algorithm's potential impact on individuals and society.  \\n• Reporting:  Audit findings will be documented in a comprehensive report, including \\nrecommendations for mitigation of any identified biases or risks.  \\n3. Data Usage Disclosures  \\n• Transparency:  We will be transparent about the data we collect, how we use it, and with \\nwhom we share it.  \\n• Privacy Policy:  Our privacy policy will clearly explain:  \\no The types of data we collect.  \\no The purposes for which we collect and use data.  \\no How we protect data privacy and security.  \\no Individuals' rights regarding their data.  \\n• Data Sharing Agreements:  Any data sharing with third parties will be governed by clear \\nand transparent data sharing agreements.  \\n4. Mechanisms for Accountability  \\n• Internal Review Board:  An internal review board comprising data scientists, ethicists, \\nand legal experts will oversee our data science operations and ensure compliance with \\nthis policy.  \\n• Ethics Hotline:  We will establish an ethics hotline for employees and stakeholders to \\nreport concerns about potential ethical violations.  \\n• Public Reporting:  We will publish an annual report summarizing our data science \\noperations, including algorithm audits, data usage, and any ethical concerns raised and \\naddressed.  \\n5. Continuous Improvement  \\n• We are committed to continuous improvement in our transparency and accountability \\npractices.  \\n• We will regularly review and update this policy to reflect evolving best practices and \\nregulatory requirements.  \\n6. Enforcement  \"),\n",
       " Document(metadata={'source': 'assignement2.pdf', 'page': 6}, page_content='• Any violation of this policy will be taken seriously and may result in disciplinary action.  \\nSources:  \\n• The AI Now Institute:  https://ainowinstitute.org/  \\n• General Data Protection Regulation (GDPR): https://gdpr.eu/  \\n \\n \\nTask 4:  Consent and Ownership of Data  \\n \\nThe below attached  form is just a  sample form. It may not have all th e fields or  attributes \\nmentioned in the below explanation . '),\n",
       " Document(metadata={'source': 'assignement2.pdf', 'page': 7}, page_content=' \\nIntroduction : \\n'),\n",
       " Document(metadata={'source': 'assignement2.pdf', 'page': 8}, page_content=\"Welcome to XYZ APP ! We're excited to have you join our community. We value your privacy \\nand want to be transparent about how we collect and use your data to personalize your \\nexperience. Please take a moment to read this consent form carefully.  \\nWhat Data We Collect ? \\nTo provide you with personalized features and recommendations, we may collect the following \\ntypes of data:  \\n• Account Information:  Your name, email address, username, and profile picture (if you \\nchoose to provide one).  \\n• Usage Data:  How you interact with the app, including the features you use, the content \\nyou view, and the time you spend on different activities.  \\n• Device Information:  Your device type, operating system, and unique device identifiers.  \\n• Location Data:  Your approximate location, if you choose to enable location services.  \\nHow We Use Your Data  \\nWe use your data to:  \\n• Personalize your experience:  Recommend content and features that are relevant to your \\ninterests.  \\n• Improve the app:  Analyze usage patterns to identify areas for improvement.  \\n• Communicate with you:  Send you updates, notifications, and promotional messages (you \\ncan opt -out of these).  \\n• Provide customer support:  Respond to your questions and resolve any issues you may \\nencounter.  \\nData Sharing  \\nWe may share your data with:  \\n• Service Providers:  Trusted third -party service providers who assist us with data storage, \\nprocessing, and analysis. These providers are contractually obligated to protect your data.  \\n• Business Partners:  Select business partners who may offer you products or services that \\ncomplement our app (you can opt -out of this sharing).  \\n• Legal Authorities:  When required by law or legal process.  \\nYour Choices  \\nYou have the following choices regarding your data:  \\n• Consent:  You can choose to consent to the collection and use of your data as described in \\nthis form.  \"),\n",
       " Document(metadata={'source': 'assignement2.pdf', 'page': 9}, page_content='• Withdrawal of Consent:  You can withdraw your consent at any time by deleting your \\naccount or contacting us.  \\n• Access and Correction:  You can access and correct your data by logging into your \\naccount settings.  \\n• Data Deletion:  You can request deletion of your data by contacting us.  \\nData Security  \\nWe take reasonable measures to protect your data from unauthorized access, use, or disclosure. \\nHowever, no method of data transmission or storage  is completely secure.     \\nChildren\\'s Privacy  \\nOur app is not intended for children under the age of 14. We do not knowingly collect data from \\nchildren.  \\nChanges to this Consent Form  \\nWe may update this consent form from time to time. We will notify you of any material changes.  \\nBy clicking \"I Agree,\" you consent to the collection and use of your data as described in \\nthis form.  \\nExplanation of How the Consent Form Addresses Ethical Concerns  \\nThis consent form is designed to meet ethical standards for clarity, voluntariness, and \\ncomprehensiveness:  \\nClarity:  \\n• Plain Language:  The form uses clear and concise language that is easy for users to \\nunderstand.  \\n• Organized Structure:  The information is presented in a well -organized and logical \\nmanner.  \\n• Visual Cues:  Headings, bullet points, and bold text are used to highlight key information.  \\nVoluntariness:  \\n• Informed Consent:  The form provides users with sufficient information to make an \\ninformed decision about whether to consent.  \\n• No Coercion:  Users are not pressured or coerced into consenting.  \\n• Easy Withdrawal:  Users can easily withdraw their consent at any time.  \\nComprehensiveness:  '),\n",
       " Document(metadata={'source': 'assignement2.pdf', 'page': 10}, page_content='• Detailed Information:  The form covers all key aspects of data collection and use, \\nincluding the types of data collected, the purposes of use, data sharing practices, and user \\nrights.  \\n• Specific Examples:  The form provides specific examples of how user data may be used.  \\n• Contact Information:  Users are provided with clear contact information in case they \\nhave questions or concerns.  \\nThis consent form aims to foster trust and transparency between the app and its users. By \\nobtaining informed and voluntary consent, we ensure that users are in control of their data and \\nthat we are using it responsibly and ethically.  \\n \\nTask 5: Ethical Dilemmas in Data Science  \\nThe Algorithmic Hiring Dilemma: Balancing Efficiency and Fairness  \\nIntroduction  \\nIn today\\'s competitive job market, organizations are increasingly turning to predictive analytics \\nto streamline their hiring processes and identify top talent.  While these technologies offer \\nsignificant benefits in terms of efficiency and cost savings, they also raise ethical concerns, \\nparticularly regarding potential bias and discrimination.  This case study examines a hypothetical \\nscenario where a tech company, \"InnovateTech,\" faces an ethical dilemma related to the use of \\npredictive analytics in hiring. W e will analyze the situation, identify the ethical challenges, and \\npropose a solution that balances the company\\'s goals with responsible data science practices.     \\nThe Scenario  \\nInnovateTech, a rapidly growing tech company, is seeking to optimize its hiring process to \\nhandle a high volume of applications for software engineering roles. The company decides to \\nimplement a predictive analytics system that analyzes resumes and applica nt data to identify \\ncandidates who are most likely to succeed in the role. The system uses machine learning \\nalgorithms trained on historical data of past successful and unsuccessful hires.     \\nWhile the system initially appears to be effective in streamlining the hiring process, concerns \\narise when an internal analysis reveals that the system disproportionately favors candidates from \\ncertain universities and backgrounds, while underrepresenting candidates from underprivileged \\ngroups and non -traditional educational pathways. This raises concerns about potential bias and \\ndiscrimination embedded within the system.  \\n \\nEthical Challenges  \\nThe use of predictive analytics in hiring at InnovateTech presents several ethical challenges:  '),\n",
       " Document(metadata={'source': 'assignement2.pdf', 'page': 11}, page_content=\"• Bias and Discrimination:  The system's tendency to favor certain demographics could \\nperpetuate existing inequalities and limit opportunities for qualified candidates from \\nunderrepresented groups.  \\n• Lack of Transparency:  The lack of transparency in how the algorithm makes decisions \\ncan make it difficult to identify and address potential biases.  \\n• Accountability:  It is unclear who is accountable for the system's decisions and any \\npotential discriminatory outcomes.  \\n• Erosion of Trust:  If candidates perceive the system as unfair or biased, it could damage \\nthe company's reputation and erode trust in its hiring practices.  \\nSolution Proposal  \\nTo address these ethical challenges, InnovateTech should adopt a multi -faceted approach that \\nbalances the benefits of predictive analytics with responsible data science practices:  \\n1. Data and Algorithm Audit:  \\no Conduct a comprehensive audit of the training data and algorithm to identify and \\nmitigate potential biases.  \\no Ensure the data is representative of the diverse pool of qualified candidates and \\ndoes not perpetuate historical inequalities.     \\no Explore fairness -aware machine learning techniques to reduce bias in the \\nalgorithm's predictions.  \\n2. Transparency and Explainability:  \\no Increase transparency by providing clear explanations of how the system works \\nand what factors are considered in candidate selection.  \\no Develop mechanisms to explain individual predictions, allowing candidates to \\nunderstand why they were or were not selected.  \\n3. Human Oversight and Accountability:  \\no Maintain human oversight in the hiring process. Use the system as a tool to assist \\nhuman decision -making, not as a replacement for it.  \\no Establish clear lines of accountability for the system's decisions and any potential \\ndiscriminatory outcomes.  \\n4. Continuous Monitoring and Improvement:  \\no Continuously monitor the system's performance for fairness and accuracy.  \\no Regularly re -train the algorithm with updated data to ensure its effectiveness and \\nmitigate evolving biases.  \"),\n",
       " Document(metadata={'source': 'assignement2.pdf', 'page': 12}, page_content=\"5. Ethical Guidelines and Training:  \\no Develop clear ethical guidelines for the use of predictive analytics in hiring.  \\no Provide training to hiring managers and data scientists on ethical considerations \\nand responsible use of the technology.  \\nSources:  \\nhttps://evontech.com/component/easyblog/ethical -considerations -in-predictive -analytics -\\nensuring -fairness -and-accountability -1-\\n1.html?Itemid=159#:~:text=If%20the%20data%20itself%20is,end%20up%20perpetuating%20th\\ne%20biases . \\n \\nhttps://www.roberthalf.com/us/en/insights/management -tips/is -predictive -analytics -the-future -of-\\nrecruitment -and-\\nhiring#:~:text=Ultimately%2C%20predictive%20analytics%20enables%20more,the%20recruitm\\nent%20and%20hiring%20processes.  \\n \\nTask 6: Impact on Society and Responsibility of Data Scientists  \\nData Scientists: Guardians of Societal Impact  \\nData scientists wield immense power in today's data -driven world. Their work shapes decisions, \\ninfluences systems, and ultimately impacts lives. This power carries an inherent responsibility to \\nconsider the broader societal implications of their work, exte nding beyond the immediate goals \\nof any project.  \\n1. Data is Not Neutral:  \\nData reflects the biases, values, and historical context of its creation. Data scientists must be \\naware of these embedded complexities to avoid perpetuating harmful biases. Ignoring the social \\nimplications of data is like ignoring the side effects of a pow erful drug – negligence can have far -\\nreaching consequences.  \\n2. Pervasive Impact:  \\nData -driven systems are now integral to healthcare, finance, education, and criminal justice. \\nAlgorithms influence decisions that impact individuals and communities profoundly. Data \\nscientists, as architects of these systems, cannot ignore their responsibi lity for the outcomes.  \\n3. Ethical Awareness is Key:  \\nThis responsibility is not about stifling innovation, but about fostering ethical awareness and \\nresponsible practice. Data scientists must be vigilant about potential biases, strive for \\ntransparency in their algorithms, and proactively assess societal impa cts. \"),\n",
       " Document(metadata={'source': 'assignement2.pdf', 'page': 13}, page_content=\"4. Specific Responsibilities:  \\n• Critical Data Awareness:  Understanding the data's origins, limitations, and potential \\nbiases is crucial.  \\n• Algorithmic Accountability:  Explainability and transparency in algorithms are essential \\nfor scrutiny and understanding.  \\n• Proactive Impact Assessment:  Considering potential societal impacts, both positive and \\nnegative, before deployment.  \\n• Ongoing Monitoring and Evaluation:  Identifying unintended consequences and ensuring \\nethical alignment over time.  \\n5. Benefits of Responsibility:  \\nEmbracing responsibility for societal impact is not just ethical, but strategic. It builds trust, \\nenhances the profession's credibility, and ensures data science serves the betterment of society.  \\nConclusion  \\nData scientists are not merely technical experts; they are ethical practitioners with a \\nresponsibility to use their skills for good. By acknowledging and addressing the societal impacts \\nof their work, they can help create a more just, equitable, and sustai nable world.  \\n \")]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading a pdf file\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('assignement2.pdf')\n",
    "pdf_documents = loader.load()\n",
    "\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://en.wikipedia.org/wiki/Y._S._Jagan_Mohan_Reddy', 'title': 'Y. S. Jagan Mohan Reddy - Wikipedia', 'language': 'en'}, page_content='\\n\\n\\n\\nY. S. Jagan Mohan Reddy - Wikipedia\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJump to content\\n\\n\\n\\n\\n\\n\\n\\nMain menu\\n\\n\\n\\n\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tNavigation\\n\\t\\n\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact us\\n\\n\\n\\n\\n\\n\\t\\tContribute\\n\\t\\n\\n\\nHelpLearn to editCommunity portalRecent changesUpload file\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDonate\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCreate account\\n\\nLog in\\n\\n\\n\\n\\n\\n\\n\\n\\nPersonal tools\\n\\n\\n\\n\\n\\n Create account Log in\\n\\n\\n\\n\\n\\n\\t\\tPages for logged out editors learn more\\n\\n\\n\\nContributionsTalk\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nContents\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n(Top)\\n\\n\\n\\n\\n\\n1\\nEarly life\\n\\n\\n\\n\\n\\n\\n\\n\\n2\\nBusiness ventures\\n\\n\\n\\n\\n\\n\\n\\n\\n3\\nPolitical career\\n\\n\\n\\n\\nToggle Political career subsection\\n\\n\\n\\n\\n\\n3.1\\n2010–2014: Founding of YSR Congress Party\\n\\n\\n\\n\\n\\n\\n\\n\\n3.2\\n2014–2019: Leader of opposition and Padayatra\\n\\n\\n\\n\\n\\n\\n\\n\\n3.3\\n2019–2024: As Chief Minister\\n\\n\\n\\n\\n\\n\\n\\n\\n3.4\\n2024–present\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n4\\nConspiracy on embezzlement charges\\n\\n\\n\\n\\n\\n\\n\\n\\n5\\nOther works\\n\\n\\n\\n\\n\\n\\n\\n\\n6\\nReferences\\n\\n\\n\\n\\n\\n\\n\\n\\n7\\nExternal links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle the table of contents\\n\\n\\n\\n\\n\\n\\n\\nY. S. Jagan Mohan Reddy\\n\\n\\n\\n17 languages\\n\\n\\n\\n\\nDeutschગુજરાતીहिन्दीBahasa Indonesiaಕನ್ನಡमैथिलीമലയാളംमराठी日本語ਪੰਜਾਬੀپنجابیᱥᱟᱱᱛᱟᱲᱤසිංහලSuomiதமிழ்తెలుగుاردو\\n\\nEdit links\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nArticleTalk\\n\\n\\n\\n\\n\\nEnglish\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nReadView sourceView history\\n\\n\\n\\n\\n\\n\\n\\nTools\\n\\n\\n\\n\\n\\nTools\\nmove to sidebar\\nhide\\n\\n\\n\\n\\t\\tActions\\n\\t\\n\\n\\nReadView sourceView history\\n\\n\\n\\n\\n\\n\\t\\tGeneral\\n\\t\\n\\n\\nWhat links hereRelated changesUpload fileSpecial pagesPermanent linkPage informationCite this pageGet shortened URLDownload QR codeWikidata item\\n\\n\\n\\n\\n\\n\\t\\tPrint/export\\n\\t\\n\\n\\nDownload as PDFPrintable version\\n\\n\\n\\n\\n\\n\\t\\tIn other projects\\n\\t\\n\\n\\nWikimedia Commons\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAppearance\\nmove to sidebar\\nhide\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFrom Wikipedia, the free encyclopedia\\n\\n\\nFormer Chief Minister of Andhra Pradesh\\n\\n\\nY. S. Jagan Mohan ReddyReddy in 2019Member of  Legislative Assembly  Andhra PradeshIncumbentAssumed office 19 June 2014[1]Preceded byY. S. VijayammaConstituencyPulivendula17th Chief Minister of Andhra PradeshIn office30 May 2019[2]\\xa0– 12 June 2024Governor\\nE. S. L. Narasimhan[3] (2019)\\nB. Harichandan[4] (2019-2023)\\nS. Abdul Nazeer[5] (2023-2024)\\nDeputy\\nK. Narayana Swamy[6](2019-2024)\\nAmzath Basha[6] (2019-2024)\\n P. Pushpasreevani[6](2019-2022)\\n P. Subhash Chandra Bose[6](2019-2020)\\n Alla Nani[6](2019-2022)\\nDharmana Krishna Das[6](2020-2022)\\nBudi Mutyala Naidu[6](2022-2024)\\nK. Sattanarayana(2022-2024)\\nRajanna Dora Peedika[6](2022-2024)\\nCabinet YS Jagan[7][8]Preceded byN. Chandrababu NaiduSucceeded byN. Chandrababu Naidu\\n\\n\\n\\nAdditional ministries\\n\\n\\nIn office30 May 2019\\xa0– 12 June 2024Ministry and Departments\\nGeneral Administration\\nPersonnel\\nOther departments not allocated to any Minister\\nPreceded byN. Chandrababu NaiduSucceeded byN. Chandrababu Naidu\\n9th Leader of the Opposition,Andhra Pradesh Legislative AssemblyIn office21 May 2014[9]\\xa0– 29 May 2019GovernorE. S. L. NarasimhanChief MinisterN. Chandrababu NaiduPreceded byN. Chandrababu NaiduSucceeded byN. Chandrababu Naidu1st President of YSR Congress PartyIncumbentAssumed office 12 March 2011ChairpersonY. S. Vijayamma (2011-2022)Preceded byPosition EstablishedMember of Parliament, Lok SabhaIn office1 June 2009[10]\\xa0– 18 May 2014Preceded byY. S. Vivekananda ReddySucceeded byY. S. Avinash ReddyConstituencyKadapa, Andhra Pradesh\\nPersonal detailsBornYeduguri Sandinti Jagan Mohan Reddy (1972-12-21) 21 December 1972 (age\\xa051)Jammalamadugu, Andhra Pradesh, IndiaPolitical partyYSR Congress PartyOther politicalaffiliationsIndian National Congress (until 2011)Spouse\\nY. S. Bharathi \\u200b(m.\\xa01996)\\u200bChildren2ParentsY. S. Rajasekhara Reddy (father)Y. S. Vijayamma (mother)Relatives\\nY. S. Sharmila(sister)\\nY. S. Avinash Reddy(cousin)\\nY. S. Vivekananda Reddy(uncle)\\nResidence(s)Vijayawada, Andhra PradeshAlma materPragathi Mahavidyalaya[11] (BCom)OccupationPolitician\\nYeduguri Sandinti Jagan Mohan Reddy (born 21 December 1972), also known mononymously as Jagan, is an Indian politician, currently serving as the Member of Legislative Assembly (MLA) representing Pulivendula Assembly constituency in the Andhra Pradesh Legislative assembly. He served as the 17th Chief Minister of Andhra Pradesh from 2019 to 2024, and is the current president of the YSR Congress Party (YSRCP). He is also the son of Y. S. Rajasekhara Reddy.\\nJagan Mohan Reddy started his political career in the Indian National Congress and was elected as the Member of Parliament of Kadapa in 2009.[12]  After his father\\'s death due to a helicopter crash in 2009, he started an Odarpu Yatra (a consoling tour) across the state.[13] He then eventually came out of the Congress Party and established his own party, YSR Congress Party which also matches his father\\'s acronym, YSR.[14]\\nOn 27 May 2012, Reddy was arrested by the Central Bureau of Investigation (CBI) on embezzlement charges. CBI summoned Reddy for allegedly amassing huge assets through illegal means by using his father\\'s office, Y. S. Rajasekhara Reddy, when he was the chief minister. CBI and ED has also summoned 58 companies of investing in Reddy\\'s businesses, for the favours they allegedly received in the form of mining leases, allotments of projects.[15] His judicial custody was extended repeatedly as the investigation proceeded.[16][17][18] The Supreme Court of India dismissed his bail petition on 4 July 2012,[19][20] 9 August 2012, 7 November 2012,[21] 9 May 2013,[22] and 13 May 2013.[23][24]\\nIn the 2014 Andhra Pradesh Legislative Assembly elections, YSRCP won 67 seats and he became the Leader of the Opposition.[25] Five years later, in 2019 Andhra Pradesh Legislative Assembly elections, he led the party to a landslide victory in the state elections by winning 151 seats of the total 175 assembly segments.[26] His party suffered severe anti-incumbency and defeat in the 2024 Andhra Pradesh legislative assembly election slumping down to a meagre 11 seats from 151 in 2019.[27][28][29]\\n\\n\\nEarly life\\nJagan Mohan Reddy was born into a Christian Reddy family in Jammalamadugu in Kadapa district of Andhra Pradesh to Y. S. Rajasekhara Reddy and Y. S. Vijayamma.[30][31] Reddy has a younger sister, Y. S. Sharmila, who is also a politician.[32]\\nHe studied at the Hyderabad Public School up to 12th grade.[31] Telugu actor Sumanth Kumar Yarlagadda was his best friend at school.[33] He graduated with a Bachelor of Commerce degree[30][31] from Pragathi Mahavidyalaya Degree and PG college,[image 1] Ram Koti, Hyderabad.[34]\\nReddy married Bharathi on 28 August 1996.[30][31] The couple has two daughters, the elder of whom studied undergraduate in London.[30][31]\\n\\nBusiness ventures\\nReddy first acquired Sandur Power Company Limited (SPCL), a defunct power project from its original promoter M B Ghorpade in 2001.[35] SPCL later invested crores of rupees in other companies and could acquire more businesses. It is headed by his wife, Y. S. Bharathi.[36] Reddy sold his shares in SPCL and moved away from his active direct businesses as he got more involved in politics.[37]\\n\\nPolitical career\\nReddy\\'s father Y. S. Rajasekhara Reddy, popularly known as YSR, was a two-time chief minister of Andhra Pradesh, serving from 2004 to 2009. He started his political career campaigning for Indian National Congress during the 2004 elections in Kadapa district.[38] In 2009, he was elected as Member of Parliament from Kadapa Lok Sabha constituency.[38]\\nSix months after his father\\'s death, he began an odarpu yatra (condolence tour), as promised earlier, to go and meet the families of those alleged to have either committed suicide or suffered ill health on the news of his father\\'s death. The Congress party\\'s central leadership directed him to call off his odarpu yatra, an order which he defied leading to a fallout between the high command and himself. He went ahead with the yatra, stating that it was a personal matter.[39]\\n\\n2010–2014: Founding of YSR Congress Party\\nFollowing the fallout with the Congress party high command, on 29 November 2010, he resigned from Kadapa Lok Sabha constituency and has also quit the party. His mother, Vijayamma, has also resigned from Pulivendula Assembly constituency and has quit the party as well.[40] He announced on 7 December 2010 from Pulivendula that he would be starting a new party within 45 days.[41] In March 2011, he announced that he would launch the new party, named YSR Congress Party, at Jaggampeta, East Godavari district.[42] Later, his party went to by-polls in Kadapa district and won almost all the seats with a huge majority.[43] Reddy, as the president of YSR Congress, faced by-election from the Kadapa constituency and won by a large margin of 545,043 votes.[44] His mother has also won the Pulivendula Assembly constituency by 85,193 votes against Y. S. Vivekananda Reddy.[45]\\n\\n2014–2019: Leader of opposition and Padayatra\\nIn 2014, the YSR Congress Party was a pre-poll favourite among most analysts and psephologists.[46] But, the YSRCP has lost the 2014 elections, winning only 67 of 175 seats in the state assembly, with 45% of vote share.[47] The Telugu Desam Party vote share went up to 47% and the 2% gap led to the defeat of YSRCP.[46]\\nAs a leader of the Opposition in Assembly and YSR Congress Party\\'s president, Reddy launched his 3,000-km-long walkathon named Praja Sankalpa Yatra, popularly called padayatra, on 6 November 2017 at Idupulapaya in Kadapa district.[48][49] YSR Congress party coined a slogan \"Raavali Jagan, Kaavali Jagan\" (transl.\\u2009Jagan should come. We want Jagan.) for the foot march that took him across 125 Assembly segments all over the state in 430 days and ended on 9 January 2019.\\nReddy while boarding a flight to Hyderabad was attacked with a cock fighting knife in the VIP Lounge of Visakhapatnam Airport on 25 October 2018.[50] He received a lacerated injury on his shoulder and had to undergo a surgery.[51]\\n\\n2019–2024: As Chief Minister\\nReddy with Prime Minister Narendra Modi\\nIn the 2019 National and State election held in April and May 2019, YSR Congress Party swept the polls and won 151 of the total 175 assembly seats and 22 of the 25 Lok Sabha seats in Andhra Pradesh. He took oath as the Chief Minister on 30 May 2019.[52] His chief ministership has been marked by a slew of welfare schemes such as Jagananna Amma Vodi, Navaratnalu.[53] Jagananna Amma Vodi provides financial assistance for mothers or guardians from the Below Poverty Line, to educate their children.[54][55] Navaratnalu is a collection of nine welfare schemes covering farmers, women, medical and health, education and Special Category Status.[53] He scrapped the plans for a new capital at Amaravati, proposed by the former TDP government, and has proposed three different capitals for the judicial, administrative and legislative branches at Kurnool, Amaravati and Visakhapatnam respectively.[56] The proposal resulted in widespread protests by the farmers of Amaravati.[57] The Andhra Pradesh High Court in a March 2022 ruling directed the Government of Andhra Pradesh to continue developing Amaravati and adjudicated that the government \"lacked the competence to make any legislation for shifting, bifurcating or trifurcating the capital\".[58]\\nAs of April 2023, it was reported by the Association for Democratic Reforms that he was the richest Chief Minister in India, with total assets of 510 crore.[59][60][61]\\n\\n2024–present\\nOn 4 June 2024, Jagan has tendered his resignation to the Governor of Andhra Pradesh after his party lost the elections.[62][63]\\nOn 12 July 2024, Jagan along with two senior IPS officers and two retired officials, were booked in an attempt to murder case, following a complaint lodged by TDP Undi MLA K Raghuram Krishna Raju.[64][65]\\n\\nConspiracy on embezzlement charges\\nThe YSR Congress Party and Reddy\\'s family have been alleging a political conspiracy behind Reddy\\'s investigations.[66][67] While in jail, Reddy started a hunger strike opposing the United Progressive Alliance\\'s decision to endorse the creation of a separate Telangana state. After 125 hours of indefinite hunger strike, his sugar levels and blood pressure were down. He was moved to Osmania General Hospital for treatment.[68][69][70] His mother, Vijayamma, was also on hunger strike protesting the formation of Telangana.[71] After his release, Reddy called for a 72-hour bandh protesting the formation of Telangana.[72] Both Reddy and his mother resigned from their legislatures opposing the decision favouring the formation of Telangana.[73]\\n\\nOther works\\nReddy founded the Telugu daily newspaper Sakshi and the television channel Sakshi TV.[74] He also served as the chief promoter of Bharathi Cements.[75]\\n\\nReferences\\n\\n\\n^ Sai, Sai (19 June 2014). \"Spotted: YS Jagan taking oath in AP Assembly\". indiaherald.com. Retrieved 6 March 2024.\\n\\n^ \"The metoric rise of YS Jagan Mohan Reddy\". WION. ANI. 30 May 2019. Retrieved 6 March 2024.\\n\\n^ Rajeev, M. (1 September 2019). \"An end to a long stint of Governor E.S.L. Narasimhan\". The Hindu. Retrieved 5 April 2024.\\n\\n^ https://www.business-standard.com/amp/article/news-ani/b-b-harichandan-anusaiya-uikey-appointed-governors-of-ap-chhattisgarh-119071601144_1.html [bare URL]\\n\\n^ \"Andhra Pradesh Chief Minister welcomes new Governor S. Abdul Nazeer\". The Hindu. 12 February 2023. Retrieved 5 April 2024.\\n\\n^ a b c d e f g h \"Andhra Pradesh Ministers: Portfolios and profiles\". The Hindu. 8 June 2019. Retrieved 5 April 2024.\\n\\n^ \"Andhra Pradesh Ministers: Portfolios and profiles\". The Hindu. 8 June 2019. Retrieved 6 April 2024.\\n\\n^ \"Andhra Pradesh cabinet expansion: 70% berths go to backwards; 14 new ministers to take oath today\". The Times of India. 11 April 2022. Retrieved 6 April 2024.\\n\\n^ \"Jagan elected leader of YSR Legislature Party\". Business Standard. 21 May 2014. Retrieved 6 March 2024.\\n\\n^ \"List of Candidates in Kadapa\\xa0: ANDHRA PRADESH Lok Sabha 2009\". My Neta. Retrieved 6 March 2024.\\n\\n^ \"Pragati Mahavidyalaya College Lecturers About Jagan | Face to Face |జగన్\\u200c..కామ్\\u200c గోయింగ్\\u200c స్టూడెంట్\\u200c\". YouTube. Sakshi TV. 31 May 2019. Retrieved 5 April 2024.\\n\\n^ Sarma, V. Ramu (28 November 2021). \"Y S Jaganmohan Reddy\\'s political journey\". www.thehansindia.com. Retrieved 8 December 2022.\\n\\n^ \"Defiant Jagan begins Odarpu yatra\". The Indian Express. 9 July 2010. Retrieved 8 December 2022.\\n\\n^ Rao, A Srinivasa (17 February 2011). \"Jaganmohan Reddy acquires YSR Congress Party from worker\". India Today. Retrieved 5 April 2024.\\n\\n^ \"CBI arrests Jagan Mohan Reddy in disproportionate assets probe\". The Economic Times. 27 May 2012. Retrieved 26 March 2021.\\n\\n^ \"CBI names Y.S. Jaganmohan Reddy as prime accused in assets case\". India Today. 7 May 2012. Retrieved 21 May 2013.\\n\\n^ Sudhir, Uma (28 May 2012). \"Jagan to stay in jail till June 11, a day before key elections\". NDTV. Retrieved 27 March 2021.\\n\\n^ \"DA case: Jagan\\'s custody extended, Sabitha appears in court\". The Telegraph. Calcutta, India. 7 June 2013. Archived from the original on 12 June 2013. Retrieved 3 November 2013.\\n\\n^ Justice Aftab Alam and Mrs. Justice Ranjana Prakash Desai (4 July 2012). \"Y.S.Jagan Mohan Reddy vs C.B.I. Anti-Corruption Branch\". Supreme Court of India.\\n\\n^ \"Jagan Reddy in SC: Can\\'t I get bail if I am wealthy? - Rediff.com News\". www.rediff.com. Retrieved 17 December 2022.\\n\\n^ Times News Network, Web Archive (5 October 2013). \"CBI court dismisses Jagan\\'s bail plea - Times Of India\". Archived from the original on 5 October 2013. Retrieved 17 December 2022.\\n\\n^ Hon\\'ble Justice P.Sathasivam, Bench: Hon\\'ble Justice M.Y. Eqbal & P Sathasivam. \"Bail denied to A1 - Y.S.Jagan Mohan Reddy vs C.B.I\". IndianKanoon.org. Retrieved 17 December 2022.\\n\\n^ Venkatesan, J. (9 August 2012). \"SC dismisses Jagan\\'s plea against arrest in DA case\". The Hindu. Chennai, India.\\n\\n^ \"CBI summons BCCI chief in Jagan case\". The Times of India. 8 June 2012. Archived from the original on 23 July 2013.\\n\\n^ \"Mere 1.68% difference of votes did Jagan\\'s party in\". The Pioneer. Retrieved 8 December 2022.\\n\\n^ Rao, Madhu (25 May 2019). \"Jagan records highest victory margin in Andhra polls\". India TV News. Retrieved 5 April 2024.\\n\\n^ MN, Samdani (5 June 2024). \"Andhra Pradesh results 2024: Jana Sena records 100% strike rate, wins 21 out of 21\". The Times of India. Retrieved 6 June 2024.\\n\\n^ \"Andhra Pradesh assembly election results 2024: What Jagan Mohan Reddy said after big defeat\". The Times of India. Retrieved 6 June 2024.\\n\\n^ Alam, Afroz (5 June 2024). \"Why Andhra Pradesh voters turned against Jagan Mohan Reddy and YSRCP\". The Indian Express. Retrieved 5 June 2024.\\n\\n^ a b c d \"Detailed Profile: Shri Y. S. Jagan Mohan Reddy\". india.gov.in. Retrieved 12 October 2019.\\n\\n^ a b c d e Bandari, Pavan Kumar (21 December 2020). \"YS Jagan Mohan Reddy Birthday: Take a look at dynamic leader\\'s journey to garner Chief Minister chair\". The Hans India. Retrieved 10 June 2021.\\n\\n^ \"Congress made a deal with TRS in return for Telangana: Jagan Reddy\\'s sister\". India Today. 2 August 2013. Retrieved 11 July 2023.\\n\\n^ Sharma, Swati (24 May 2019). \"Jagan Mohan Reddy makes Hyderabad Public School proud\". Deccan Chronicle. Retrieved 17 December 2022.\\n\\n^ \"Jagan studied in this degree college: Friend & Principal Before TV9 On A1-JMR\". ap7am.com. TV9 & AP7AM. Retrieved 17 December 2022.\\n\\n^ \"How Jagan Reddy became the richest Lok Sabha MP in India, and what is his real worth?\". India Today. 28 May 2012.\\n\\n^ \"Ghost investors, Luxembourg slush cash built Jagan Mohan Reddy\\'s \\'billions\\', says CBI\". India Today. Living Media India Limited. India Today and CBI. Retrieved 17 December 2022.\\n\\n^ Solanki, Damini (13 April 2023). \"Celebrity Education: India\\'s Richest CM Jagan Mohan Reddy is a BCom Graduate\". News18. Retrieved 7 June 2024.\\n\\n^ a b Shanker, M. S. (24 May 2019). \"YS Jaganmohan Reddy – Andhra\\'s Giant Killer\". Outlook. India. Retrieved 28 June 2019.\\n\\n^ \"Defiant Jagan to go ahead with \\'Odarpu\\' yatra\". The Times of India. PTI. 22 August 2010. Retrieved 27 March 2021.\\n\\n^ \"Jagan quits Congress, Kadapa Lok Sabha seat\". The Hindu. 29 November 2010. Retrieved 26 March 2021.\\n\\n^ \"Jaganmohan Reddy to launch new party within 45 days\". The Times of India. 7 December 2010. Retrieved 26 March 2021.\\n\\n^ \"Jagan to Launch YSR Congress Party on March 12\". Outlook. 11 March 2011. Retrieved 26 March 2021.\\n\\n^ \"YSR Congress sweeps AP by-polls; wins 15 assembly seats, 1 LS seat\". DNA. 15 June 2012. Retrieved 27 March 2021.\\n\\n^ Menon, Amarnath K (13 May 2011). \"Kadapa bypoll: Jagan wins by 5,43,053 votes\". India Today. Retrieved 27 March 2021.\\n\\n^ \"Kadapa bypoll: Jagan Mohan Reddy wins by 5,43,053 votes\". The Times of India. 13 May 2011. Retrieved 26 March 2021.\\n\\n^ a b \"Why Congress and YSRCP lost in Telangana and Andhra Pradesh\". 20 May 2014.\\n\\n^ Kalavalapalli, Yogendra (20 May 2014). \"Why Congress and YSRCP lost in Telangana and Andhra Pradesh\". Mint.\\n\\n^ Pandey, Ashish (4 November 2017). \"YS Jagan all set to embark on his 3000 KM long Padyatara in Andhra Pradesh\". India Today. Retrieved 26 March 2021.\\n\\n^ \"Jagan Mohan Reddy\\'s Praja Sankalpa Yatra completes 199 days, all set to create a record\". The New Indian Express. 27 June 2018. Retrieved 26 March 2021.\\n\\n^ Pandey, Ashish; Sandhu, Kamaljit Kaur (25 October 2019). \"Man stabs Jagan Mohan Reddy of YSR Congress at airport while taking a selfie with him\". India Today. Retrieved 20 November 2019.\\n\\n^ \"Jaganmohan Reddy Discharged From Hospital After Knife Attack\". NDTV. 28 October 2018. Retrieved 26 March 2021.\\n\\n^ \"Jagan-naut: Andhra\\'s bahubali Jaganmohan Reddy takes oath as CM in grand ceremony at Vijayawada\". India Today. 30 May 2019. Retrieved 26 March 2021.\\n\\n^ a b \"Navaratnalu, welfare get the lion\\'s share of YSRCP\\'s maiden budget\". The Hindu. 13 July 2019. ISSN\\xa00971-751X. Retrieved 20 June 2021.\\n\\n^ Srinivas, Rajulapudi (9 March 2021). \"\\'Jagananna Amma Vodi\\' gives women a reason to cheer\". The Hindu. ISSN\\xa00971-751X. Retrieved 20 June 2021.\\n\\n^ \"Andhra Pradesh government spends Rs. 25,714 crore on education\". Deccan Express. 22 May 2021. Retrieved 20 June 2021.\\n\\n^ Apparasu, Srinivasa Rao (15 December 2020). \"Jagan meets Shah, asks to begin process of shifting HC to Kurnool as per 3 capitals plan\". Hindustan Times. Retrieved 21 December 2020.\\n\\n^ Sudhir, Uma (13 January 2020). \"Won\\'t Celebrate Harvest Festival, Say Amaravati Farmers Amid Protests\". NDTV. Retrieved 28 February 2021.\\n\\n^ Jonathan, P. Samuel (4 March 2022). \"Andhra Pradesh Government can\\'t change capital: High Court\". The Hindu. ISSN\\xa00971-751X. Retrieved 7 April 2022.\\n\\n^ \"Richest Chief Minister in India 2023 List: Who Is the Richest CM? Which CM Has the Lowest Total Assets According to ADR Survey Report? Check All The Names | Association for Democratic Reforms\". adrindia.org. Retrieved 16 May 2023.\\n\\n^ \"Jagan Mohan Reddy wealthiest CM, Mamata Banerjee least well-off: ADR report\". The Indian Express. 13 April 2023. Retrieved 16 May 2023.\\n\\n^ \"Andhra Pradesh\\'s Jagan is India\\'s wealthiest CM, West Bengal\\'s Mamata least well-off: ADR report\". The Times of India. 13 April 2023. ISSN\\xa00971-8257. Retrieved 16 May 2023.\\n\\n^ \"Jagan Mohan Reddy Resigns As Andhra Pradesh Chief Minister, Sends Resignation To Governor\". NDTV. 4 June 2024. Retrieved 7 June 2024.\\n\\n^ \"Andhra Pradesh Election Results 2024 Live: Andhra Governor accepts Jagan\\'s resignation, requests him to continue till new govt is formed\". The Economic Times. 4 June 2024. Retrieved 7 June 2024.\\n\\n^ \"Former Andhra CM Jagan Mohan Reddy, two IPS officers booked in \\'attempt to murder\\' case\". The Times of India. 12 July 2024. ISSN\\xa00971-8257. Retrieved 15 July 2024.\\n\\n^ \"Attempt to murder case filed against Jagan Reddy, former IPS officers\". India Today. 12 July 2024. Retrieved 15 July 2024.\\n\\n^ \"YSR Cong cries foul over CBI\\'s \\'selective probe\\'\". The Times of India. 14 May 2012. Retrieved 27 March 2021.\\n\\n^ \"Vijayamma and Bharathi lash out at CBI on Jagan probe\". Times AP. Archived from the original on 13 August 2014. Retrieved 4 October 2013.\\n\\n^ \"Jagan Shifted to OGH\". Indistan News. 29 August 2013. Archived from the original on 10 March 2014. Retrieved 3 November 2013.\\n\\n^ \"Jagan shifted to Osmania Hospital\". The Hindu. 29 August 2013.\\n\\n^ \"Jagan shifted to Osmania Hospital\". www.thehansindia.com. 30 August 2013. Retrieved 17 December 2022.\\n\\n^ \"Jagan Mohan Reddy\\'s mother Vijayamma continues hunger strike in hospital\". NDTV. Indo-Asian News Service. 24 August 2013.\\n\\n^ \"Telangana: Y S Jaganmohan Reddy blasts Centre, calls for 72 hour bandh\". The Economic Times. Press Trust of India. 4 October 2013. Retrieved 4 October 2013.\\n\\n^ \"Y S Jaganmohan Reddy resigns as MP over AP split; his mother quits Assembly\". The Indian Express. 10 August 2013.\\n\\n^ \"Archived copy\" (PDF). Archived from the original (PDF) on 13 August 2011. Retrieved 5 February 2011.{{cite news}}:  CS1 maint: archived copy as title (link)\\n\\n^ Venkatesha Babu and C.R. Sukumar (20 April 2010). \"France\\'s Vicat buys 51% stake in Bharathi Cement\". Livemint. Retrieved 21 May 2013.\\n\\n\\n\\n\\n^ \"Pragathi Mahavidyalaya, Koti, Hyderabad\". Photo, 29 March 1994. Pragathi Mahavidyalaya, A1-JMR Register Sr. No. 4734/A - B.Com First Class Student - College Leaving Certificate, 1994.\\n\\n\\nExternal links\\n\\n\\n\\nWikimedia Commons has media related to Y. S. Jaganmohan Reddy.\\n\\nOfficial website\\n\\n\\nLok Sabha\\n\\n\\nPreceded\\xa0byY. S. Vivekananda Reddy\\n\\n Member of Parliamentfor Kadapa 2009–2014\\n\\nSucceeded\\xa0byY. S. Avinash Reddy\\n\\n\\nPolitical offices\\n\\n\\nPreceded\\xa0byN. Chandrababu Naidu\\n\\n Chief minister of Andhra Pradesh 30 May 2019 – 11 June 2024\\n\\nSucceeded\\xa0byN. Chandrababu Naidu\\n\\n\\nParty political offices\\n\\n\\nPreceded\\xa0byParty did not exist\\n\\n Leader of the YSR Congress Party in the 15th Lok Sabha 2011–2014\\n\\nSucceeded\\xa0byMekapati Rajamohan Reddy\\n\\n\\nvteChief ministers of Andhra PradeshAndhra state\\nTanguturi Prakasam\\nBezawada Gopala Reddy\\nAndhra Pradesh\\nNeelam Sanjiva Reddy\\nDamodaram Sanjivayya\\nKasu Brahmananda Reddy\\nP. V. Narasimha Rao\\nJalagam Vengala Rao\\nMarri Chenna Reddy\\nT. Anjaiah\\nBhavanam Venkatarami Reddy\\nKotla Vijaya Bhaskara Reddy\\nN. T. Rama Rao (Chief ministership)\\nN. Bhaskara Rao\\nN. Janardhana Reddy\\nY. S. Rajasekhara Reddy\\nKonijeti Rosaiah\\nKiran Kumar Reddy\\nY. S. Jagan Mohan Reddy\\nN. Chandrababu Naidu (Chief ministership)\\n\\n\\n\\nY. S. Rajasekhara Reddy family\\n\\n\\n\\n\\nY. S. GangammaY. S. Venkat ReddyY. S. Mangamma\\n\\nY. S. Chinna KondareddyY. S. Pedda KondareddyY. S. SugunammaY. S. Prabhudas ReddyY. S. RatnammaY. S. Raja ReddyY. S. JayammaY. S. Purushottam ReddyY. S. Mary PuneetammaY. S. Kamalamma\\n\\nY. S. Rajasekhara ReddyY. S. VijayammaY. S. Sudheekar ReddyY. S. VidhyaJayaY. S. Ravindranath ReddyY. S. Vivekananda ReddyY. S. Soubhagyamma\\n\\nY. S. SharmilaM. Anil KumarY. S. Jagan Mohan ReddyY. S. BharathiY. S. Viranica ReddyManchu VishnuY. S. SuneethaNarreddy Rajasekhar Reddy\\n\\n\\n\\n\\n\\n\\nRetrieved from \"https://en.wikipedia.org/w/index.php?title=Y._S._Jagan_Mohan_Reddy&oldid=1249693259\"\\nCategories: Living people1972 birthsIndian AnglicansPeople from Kadapa districtIndia MPs 2009–2014Chief ministers of Andhra PradeshTelugu politiciansGovernment of Andhra PradeshPeople from RayalaseemaIndian National Congress politicians from Andhra PradeshLok Sabha members from Andhra PradeshIndian fraudstersLeaders of the Opposition in the Andhra Pradesh Legislative AssemblyIndian ChristiansIndian political party foundersYSR Congress Party politiciansPeople charged with corruptionPrisoners and detainees of IndiaYSR Congress PartyAndhra Pradesh MLAs 2014–2019Andhra Pradesh MLAs 2019–2024Andhra Pradesh MLAs 2024–2029People named in the Paradise PapersIndian politicians convicted of corruptionHidden categories: All articles with bare URLs for citationsArticles with bare URLs for citations from August 2024CS1 maint: archived copy as titleArticles with short descriptionShort description is different from WikidataWikipedia indefinitely semi-protected biographies of living peopleUse Indian English from July 2024All Wikipedia articles written in Indian EnglishUse dmy dates from July 2024Commons category link from WikidataOfficial website not in Wikidata\\n\\n\\n\\n\\n\\n\\n This page was last edited on 6 October 2024, at 09:39\\xa0(UTC).\\nText is available under the Creative Commons Attribution-ShareAlike License 4.0;\\nadditional terms may apply. By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia® is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\\n\\n\\nPrivacy policy\\nAbout Wikipedia\\nDisclaimers\\nContact Wikipedia\\nCode of Conduct\\nDevelopers\\nStatistics\\nCookie statement\\nMobile view\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]\n"
     ]
    }
   ],
   "source": [
    "##web based loader\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "# Correct the URL in web_paths\n",
    "loader = WebBaseLoader(web_paths=('https://en.wikipedia.org/wiki/Y._S._Jagan_Mohan_Reddy',))  \n",
    "\n",
    "web_documents = loader.load()\n",
    "print(web_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Arxiv loader\n",
    "\n",
    "from langchain_community.document_loaders import ArxivLoader\n",
    "\n",
    "loader = ArxivLoader(query = '1706.03762', load_max_docs = 2)\n",
    "arxiv_documents = loader.load()\n",
    "\n",
    "len(arxiv_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'Published': '2023-08-02', 'Title': 'Attention Is All You Need', 'Authors': 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin', 'Summary': 'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and limited training data.'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\\n1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2\\nFigure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3\\nScaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4\\noutput values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5\\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6\\nlength n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7\\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8\\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9\\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10\\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11\\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12\\nAttention Visualizations\\nInput-Input Layer5\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nInput-Input Layer5\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15\\n')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'title': 'Y. S. Jagan Mohan Reddy', 'summary': \"Yeduguri Sandinti Jagan Mohan Reddy (born 21 December 1972), also known mononymously as Jagan, is an Indian politician, currently serving as the Member of Legislative Assembly (MLA) representing Pulivendula Assembly constituency in the Andhra Pradesh Legislative assembly. He served as the 17th Chief Minister of Andhra Pradesh from 2019 to 2024, and is the current president of the YSR Congress Party (YSRCP). He is also the son of Y. S. Rajasekhara Reddy.\\nJagan Mohan Reddy started his political career in the Indian National Congress and was elected as the Member of Parliament of Kadapa in 2009.  After his father's death due to a helicopter crash in 2009, he started an Odarpu Yatra (a consoling tour) across the state. He then eventually came out of the Congress Party and established his own party, YSR Congress Party which also matches his father's acronym, YSR.\\nOn 27 May 2012, Reddy was arrested by the Central Bureau of Investigation (CBI) on embezzlement charges. CBI summoned Reddy for allegedly amassing huge assets through illegal means by using his father's office, Y. S. Rajasekhara Reddy, when he was the chief minister. CBI and ED has also summoned 58 companies of investing in Reddy's businesses, for the favours they allegedly received in the form of mining leases, allotments of projects. His judicial custody was extended repeatedly as the investigation proceeded. The Supreme Court of India dismissed his bail petition on 4 July 2012, 9 August 2012, 7 November 2012, 9 May 2013, and 13 May 2013.\\nIn the 2014 Andhra Pradesh Legislative Assembly elections, YSRCP won 67 seats and he became the Leader of the Opposition. Five years later, in 2019 Andhra Pradesh Legislative Assembly elections, he led the party to a landslide victory in the state elections by winning 151 seats of the total 175 assembly segments. His party suffered severe anti-incumbency and defeat in the 2024 Andhra Pradesh legislative assembly election slumping down to a meagre 11 seats from 151 in 2019.\\n\\n\", 'source': 'https://en.wikipedia.org/wiki/Y._S._Jagan_Mohan_Reddy'}, page_content=\"Yeduguri Sandinti Jagan Mohan Reddy (born 21 December 1972), also known mononymously as Jagan, is an Indian politician, currently serving as the Member of Legislative Assembly (MLA) representing Pulivendula Assembly constituency in the Andhra Pradesh Legislative assembly. He served as the 17th Chief Minister of Andhra Pradesh from 2019 to 2024, and is the current president of the YSR Congress Party (YSRCP). He is also the son of Y. S. Rajasekhara Reddy.\\nJagan Mohan Reddy started his political career in the Indian National Congress and was elected as the Member of Parliament of Kadapa in 2009.  After his father's death due to a helicopter crash in 2009, he started an Odarpu Yatra (a consoling tour) across the state. He then eventually came out of the Congress Party and established his own party, YSR Congress Party which also matches his father's acronym, YSR.\\nOn 27 May 2012, Reddy was arrested by the Central Bureau of Investigation (CBI) on embezzlement charges. CBI summoned Reddy for allegedly amassing huge assets through illegal means by using his father's office, Y. S. Rajasekhara Reddy, when he was the chief minister. CBI and ED has also summoned 58 companies of investing in Reddy's businesses, for the favours they allegedly received in the form of mining leases, allotments of projects. His judicial custody was extended repeatedly as the investigation proceeded. The Supreme Court of India dismissed his bail petition on 4 July 2012, 9 August 2012, 7 November 2012, 9 May 2013, and 13 May 2013.\\nIn the 2014 Andhra Pradesh Legislative Assembly elections, YSRCP won 67 seats and he became the Leader of the Opposition. Five years later, in 2019 Andhra Pradesh Legislative Assembly elections, he led the party to a landslide victory in the state elections by winning 151 seats of the total 175 assembly segments. His party suffered severe anti-incumbency and defeat in the 2024 Andhra Pradesh legislative assembly election slumping down to a meagre 11 seats from 151 in 2019.\\n\\n\\n== Early life ==\\nJagan Mohan Reddy was born into a Christian Reddy family in Jammalamadugu in Kadapa district of Andhra Pradesh to Y. S. Rajasekhara Reddy and Y. S. Vijayamma. Reddy has a younger sister, Y. S. Sharmila, who is also a politician.\\nHe studied at the Hyderabad Public School up to 12th grade. Telugu actor Sumanth Kumar Yarlagadda was his best friend at school. He graduated with a Bachelor of Commerce degree from Pragathi Mahavidyalaya Degree and PG college, Ram Koti, Hyderabad.\\nReddy married Bharathi on 28 August 1996. The couple has two daughters, the elder of whom studied undergraduate in London.\\n\\n\\n== Business ventures ==\\nReddy first acquired Sandur Power Company Limited (SPCL), a defunct power project from its original promoter M B Ghorpade in 2001. SPCL later invested crores of rupees in other companies and could acquire more businesses. It is headed by his wife, Y. S. Bharathi. Reddy sold his shares in SPCL and moved away from his active direct businesses as he got more involved in politics.\\n\\n\\n== Political career ==\\nReddy's father Y. S. Rajasekhara Reddy, popularly known as YSR, was a two-time chief minister of Andhra Pradesh, serving from 2004 to 2009. He started his political career campaigning for Indian National Congress during the 2004 elections in Kadapa district. In 2009, he was elected as Member of Parliament from Kadapa Lok Sabha constituency.\\nSix months after his father's death, he began an odarpu yatra (condolence tour), as promised earlier, to go and meet the families of those alleged to have either committed suicide or suffered ill health on the news of his father's death. The Congress party's central leadership directed him to call off his odarpu yatra, an order which he defied leading to a fallout between the high command and himself. He went ahead with the yatra, stating that it was a personal matter.\\n\\n\\n=== 2010–2014: Founding of YSR Congress Party ===\\nFollowing the fallout with the Congress party high command, on 29 November 2010, h\"), Document(metadata={'title': 'Y. S. Jagan Mohan Reddy ministry', 'summary': 'The Y. S. Jagan Mohan Reddy ministry (or also known as 27th ministry of Andhra Pradesh) of the state of Andhra Pradesh formed the executive branch of the government of Andhra Pradesh. Along with the chief minister, there are 5 deputy chief ministers and 20 cabinet ministers.\\n\\n', 'source': 'https://en.wikipedia.org/wiki/Y._S._Jagan_Mohan_Reddy_ministry'}, page_content='The Y. S. Jagan Mohan Reddy ministry (or also known as 27th ministry of Andhra Pradesh) of the state of Andhra Pradesh formed the executive branch of the government of Andhra Pradesh. Along with the chief minister, there are 5 deputy chief ministers and 20 cabinet ministers.\\n\\n\\n== Cabinet Ministers ==\\n\\n\\n== Ministers by District ==\\n\\n\\n== Previous Cabinet Ministers ==\\n\\n\\n== References ==')]\n"
     ]
    }
   ],
   "source": [
    "# wikipedia loader\n",
    "\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "loader = WikipediaLoader(query='Y. S. Jagan Mohan Reddy', load_max_docs=2)\n",
    "wiki_documents = loader.load()\n",
    "\n",
    "print(wiki_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

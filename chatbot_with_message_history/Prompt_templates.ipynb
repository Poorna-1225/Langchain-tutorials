{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt Templates help to run raw user information into a format that LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Prompt Template:\n",
    "\n",
    "`prompt = ChatPromptTemplate.from_messages(...):` This creates a ChatPromptTemplate object. Prompt templates help you structure your prompts consistently. This specific template defines two parts:\n",
    "* `('system', 'Hey you are a helpful assistant. Answer all the questions to the best of your ability')`: This sets the \"system\" role in the chat, providing an initial instruction to the language model.\n",
    "* `MessagesPlaceholder(variable_name=\"messages\")`: This is where the magic of MessagePlaceholder comes in. It creates a placeholder within your prompt template where you can dynamically insert a list of messages. This is crucial for multi-turn conversations where you want to include the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-4o')\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','Hey you are a helpful assitant. Answer all the questions to the nest of your ability'),\n",
    "        MessagesPlaceholder(variable_name=\"messages\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt|llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello Poorna! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 35, 'total_tokens': 46, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6b68a8204b', 'finish_reason': 'stop', 'logprobs': None}, id='run-98c4c65a-dcb2-49e2-b3fe-76c8935dd7f3-0', usage_metadata={'input_tokens': 35, 'output_tokens': 11, 'total_tokens': 46, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'messages': [HumanMessage(content='Hi my name is poorna')]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "store={}\n",
    "def get_session_history(session_id:str)-> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(chain, get_session_history=get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello, Poorna! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 35, 'total_tokens': 47, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_6b68a8204b', 'finish_reason': 'stop', 'logprobs': None}, id='run-fafc9e6c-df2d-4636-8912-4e15fd62d6e1-0', usage_metadata={'input_tokens': 35, 'output_tokens': 12, 'total_tokens': 47, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {'configurable':{'session_id':'chat3'}}\n",
    "\n",
    "response = with_message_history.invoke(\n",
    "    [HumanMessage(content = 'Hi my name is poorna')],\n",
    "    config = config\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'హలో పూర్ణ, మీరు ఎలా ఉన్నారు? నాకు సహాయం అవసరమైతే చెప్పండి!'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding more complexity\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [('system','you are a helpful assistant. Answer all questions to the best of your ability in {language}'),\n",
    "     MessagesPlaceholder(variable_name='messages')]\n",
    ")\n",
    "\n",
    "chain = prompt|llm\n",
    "\n",
    "\n",
    "response  = chain.invoke(\n",
    "    {\n",
    "        'messages' : [HumanMessage(content ='Hi my name is poorna')],\n",
    "        'language': 'Telugu'\n",
    "        }\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunnableWithMessageHistory` is a class in LangChain that helps manage conversation history when you're interacting with a language model. It acts as a wrapper around another `Runnable` object (like an LLM or a Chain) and takes care of storing and retrieving messages from the conversation.\n",
    "\n",
    "Here's a breakdown of what it does:\n",
    "\n",
    "**1. Storing Messages:**\n",
    "\n",
    "- Every time you run the `RunnableWithMessageHistory`, it automatically stores the input and output messages in a `ChatMessageHistory` object. This history can be stored in memory or in a persistent store like a database.\n",
    "\n",
    "**2. Retrieving Messages:**\n",
    "\n",
    "- When you invoke the wrapped `Runnable` again, `RunnableWithMessageHistory` retrieves the relevant message history and includes it in the prompt. This ensures that the language model has context from the previous turns of the conversation.\n",
    "\n",
    "**Why is this useful?**\n",
    "\n",
    "- **Maintaining Context:**  Language models have no memory of past interactions. `RunnableWithMessageHistory` provides this memory by feeding the conversation history back into the model, enabling more coherent and contextually relevant responses.\n",
    "- **Simplifying Conversation Handling:** You don't have to manually manage message storage and retrieval. `RunnableWithMessageHistory` handles this for you, making it easier to build conversational applications.\n",
    "\n",
    "**How to Use It:**\n",
    "\n",
    "```python\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.runnables import RunnableWithMessageHistory\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize your LLM and prompt\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "        HumanMessage(content=\"{input}\"),\n",
    "    ]\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Create a memory object to store the history\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# Wrap the chain with RunnableWithMessageHistory\n",
    "runnable_with_history = RunnableWithMessageHistory(\n",
    "    runnable=chain, \n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Start the conversation\n",
    "runnable_with_history.invoke({\"input\": \"Hi, my name is Bob\"})  \n",
    "runnable_with_history.invoke({\"input\": \"What's my name?\"}) # The LLM will remember!\n",
    "```\n",
    "\n",
    "In this example, `RunnableWithMessageHistory` manages the conversation history between the user (\"Bob\") and the LLM. The second time you invoke it, the LLM will \"remember\" the previous interaction and correctly identify Bob's name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'హలో పూర్ణ! మీతో పరిచయం కావడం ఆనందంగా ఉంది. మీకు ఎలా సహాయపడగలను?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are a helpful assistant.Answer all the questions in {language}'),\n",
    "        MessagesPlaceholder(variable_name='messages')\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt|llm\n",
    "\n",
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key= 'messages'\n",
    ")\n",
    "\n",
    "config = {'configurable':{'session_id':'chat4'}}\n",
    "\n",
    "response =  with_message_history.invoke(\n",
    "    {'messages': [HumanMessage(content ='Hi my name is poorna')],\n",
    "     \"language\":'telugu'},\n",
    "     config =  config\n",
    ")\n",
    "\n",
    "response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing the conversation history\n",
    "\n",
    "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`trim_messages` is a very useful function in LangChain, especially when building chatbots that involve longer conversations. It helps you manage the context window of language models by trimming the conversation history to a manageable size.\n",
    "\n",
    "Here's why it's important and how it works:\n",
    "\n",
    "**The Context Window Problem:**\n",
    "\n",
    "* Language models (LLMs) have a limited context window – they can only \"remember\" a certain amount of text from the current conversation.\n",
    "* As a conversation gets longer, the message history can exceed this limit.\n",
    "* If you send too much history to the LLM, it might lose track of the earlier parts of the conversation or even exceed the maximum token limit, resulting in errors.\n",
    "\n",
    "**How `trim_messages` Helps:**\n",
    "\n",
    "* `trim_messages` allows you to truncate the conversation history while preserving the most important information.\n",
    "* You can specify how many tokens or messages to keep, and which strategy to use for trimming (e.g., keep the most recent messages, keep the first few messages, etc.).\n",
    "* This ensures that the LLM receives a concise and relevant history, preventing context window overflow and improving performance.\n",
    "\n",
    "**Key Benefits:**\n",
    "\n",
    "* **Improved Performance:**  Avoids exceeding the LLM's token limits, leading to faster responses and fewer errors.\n",
    "* **Reduced Costs:**  Shorter prompts mean fewer tokens are processed, potentially lowering the cost of using the LLM.\n",
    "* **Better Context:**  By keeping the most relevant parts of the conversation, you can ensure the LLM has the necessary context to provide meaningful responses.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain.messages import trim_messages\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful AI assistant.\"),\n",
    "    HumanMessage(content=\"Hi, what's the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"And what's the population of Paris?\"),\n",
    "    AIMessage(content=\"The population of Paris is about 2.1 million.\"),\n",
    "    # ... more messages ...\n",
    "]\n",
    "\n",
    "# Trim the messages to the last 50 tokens\n",
    "trimmed_messages = trim_messages(messages, max_tokens=50, token_counter=llm) \n",
    "```\n",
    "\n",
    "In this example, `trim_messages` will shorten the conversation history to fit within 50 tokens, likely keeping the most recent messages to provide context for the ongoing conversation.\n",
    "\n",
    "**Important Considerations:**\n",
    "\n",
    "* **Trimming Strategy:** Choose the right strategy (`\"first\"`, `\"last\"`) based on your chatbot's needs.\n",
    "* **Token Counting:**  Ensure you have an accurate way to count tokens (e.g., using the LLM's tokenizer).\n",
    "* **System Messages:** Decide whether to include system messages in the trimming process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='who is YS Jagan Mohan Reddy?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='YS Jagan Mohan Reddy is a renowned Indian politician.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Tell me more about the latest match', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The latest match is between India and Australia.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Can you help me find a restaurant near me?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='I recommend the Chennai Tandoor.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import SystemMessage, trim_messages\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "trimmer = trim_messages(\n",
    "    max_tokens = 100,\n",
    "    strategy = \"last\",\n",
    "    token_counter = llm,\n",
    "    include_system = True,\n",
    "    allow_partial = False,\n",
    "    start_on = 'human'\n",
    "    )\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content =\"you're a good assistant\"),\n",
    "    HumanMessage(content=\"Hi, how's the weather today?\"),\n",
    "    AIMessage(content=\"The weather is sunny with a temperature of 25°C.\"),\n",
    "    HumanMessage(content=\"What's the current news?\"),\n",
    "    AIMessage(content=\"The latest news is about the coronavirus outbreak.\"),\n",
    "    HumanMessage(content=\"who is YS Jagan Mohan Reddy?\"),\n",
    "    AIMessage(content=\"YS Jagan Mohan Reddy is a renowned Indian politician.\"),\n",
    "    HumanMessage(content=\"Tell me more about the latest match\"),\n",
    "    AIMessage(content=\"The latest match is between India and Australia.\"),\n",
    "    HumanMessage(content=\"Can you help me find a restaurant near me?\"),\n",
    "    AIMessage(content=\"I recommend the Chennai Tandoor.\"),\n",
    "    \n",
    "]\n",
    "\n",
    "trimmer.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'వైఎస్ జగన్ మోహన్ రెడ్డి ఆంధ్రప్రదేశ్ రాష్ట్ర ముఖ్యమంత్రిగా ఉన్నారు. ఆయన 2019లో జరిగిన ఎన్నికల్లో వైఎస్సార్ కాంగ్రెస్ పార్టీ తరఫున ముఖ్యమంత్రి పదవిని సాధించారు. ఆయన అంతకు ముందు దివంగత ముఖ్యమంత్రి వైఎస్ రాజశేఖర రెడ్డి కుమారుడు. వైఎస్ జగన్ మోహన్ రెడ్డి ఆంధ్రప్రదేశ్ రాష్ట్రంలో అనేక సంక్షేమ కార్యక్రమాలను ప్రారంభించారు మరియు రాష్ట్ర అభివృద్ధి కోసం కృషి చేస్తున్నారు.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system','you are a helpful assistant.Answer all the questions in {language}'),\n",
    "        MessagesPlaceholder(variable_name='messages')\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(messages = itemgetter('messages')|trimmer)\n",
    "    |prompt\n",
    "    |llm\n",
    ")\n",
    "\n",
    "response = chain.invoke({\n",
    "    'messages': messages +[HumanMessage(content=\"Who is YS Jagan Mohan Reddy?\")],\n",
    "    'language': 'telugu'}\n",
    "    )\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'వైఎస్ జగన్ మోహన్ రెడ్డి ఆంధ్రప్రదేశ్ రాష్ట్రానికి ముఖ్యమంత్రిగా ఉన్నారు. ఆయన 2019 మే 30న ముఖ్యమంత్రిగా బాధ్యతలు స్వీకరించారు. జగన్ మోహన్ రెడ్డి వై ఎస్ ఆర్ కాంగ్రెస్ పార్టీకి అధినేతగా ఉన్నారు. ఆయన ఆంధ్రప్రదేశ్ మాజీ ముఖ్యమంత్రి డాక్టర్ వై.ఎస్. రాజశేఖర రెడ్డి గారి కుమారుడు. రాజకీయాల్లోకి రాక ముందు ఆయన వ్యాపార రంగంలో కూడా ఉన్నారు.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key= 'messages')\n",
    "\n",
    "config = {'configurable':{'session_id':'chat5'}}\n",
    "response = with_message_history.invoke(\n",
    "    {\n",
    "        'messages': messages +[HumanMessage(content=\"Who is YS Jagan Mohan Reddy?\")],\n",
    "    'language': 'telugu'},\n",
    "config = config\n",
    ")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors and Retrivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents\n",
    "\n",
    "You're absolutely right! It seems I got carried away with the broader concept of \"documents\" in LangChain. \n",
    "\n",
    "You're specifically interested in the `Document` class within `langchain_core.documents`. \n",
    "\n",
    "Here's a breakdown of the `langchain_core.documents.Document` class:\n",
    "\n",
    "**Purpose:**\n",
    "\n",
    "The `Document` class is a fundamental building block for representing pieces of text within LangChain. It provides a standardized way to store and work with text data along with its associated metadata.\n",
    "\n",
    "**Key Components:**\n",
    "\n",
    "* **page_content:** This attribute stores the actual text content of the document. It can be a string of any length, representing an article, code snippet, email, etc.\n",
    "* **metadata:** This is a dictionary that holds additional information about the document. This metadata can be crucial for:\n",
    "    * **Source Tracking:**  Store the origin of the document (URL, file path, database ID).\n",
    "    * **Contextualization:**  Include information like author, date, or category.\n",
    "    * **Document Relationships:**  Define links or connections to other documents.\n",
    "    * **Custom Properties:**  Add any other relevant information you need.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "doc = Document(\n",
    "    page_content=\"This is an example document.\",\n",
    "    metadata={\"source\": \"example.txt\", \"author\": \"John Doe\"}\n",
    ")\n",
    "```\n",
    "\n",
    "**Why is this important?**\n",
    "\n",
    "* **Organization:**  Provides a structured way to manage text data.\n",
    "* **Contextualization:**  Metadata adds valuable context to the text, which can be used for retrieval or to guide the language model.\n",
    "* **Integration:**  The `Document` class is used throughout LangChain for various tasks like:\n",
    "    * **Loading data:**  Document loaders return lists of `Document` objects.\n",
    "    * **Splitting text:**  Text splitters operate on `Document` objects.\n",
    "    * **Storing data:**  Document stores often use `Document` objects as their basic unit.\n",
    "    * **Creating prompts:** You can include `Document` content and metadata in prompts to provide context to the LLM.\n",
    "\n",
    "**Key Takeaway:**\n",
    "\n",
    "The `langchain_core.documents.Document` class is a fundamental element in LangChain for representing and working with text data and its associated metadata. It plays a crucial role in organizing, contextualizing, and processing text information within your LLM applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content= \"Dogs are great companions, known for their loyality and friendliness.\",\n",
    "        metadata ={'source':\"mammal-pets-doc\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content= \"Cats are intelligent and playful creatures, often seen as a symbol of companionship and happiness.\",\n",
    "        metadata = {'source':\"mammal-pets-doc\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content= \"Birds are fascinating creatures with a wide range of behaviors and species.\",\n",
    "        metadata = {'source':\"bird-behaviour-doc\"} ),\n",
    "    Document(\n",
    "        page_content= \"Fish are aquatic animals that are omnivores, meaning they eat a variety of foods.\",\n",
    "        metadata = {'source':\"aquatic-animals-doc\"}\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyality and friendliness.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are intelligent and playful creatures, often seen as a symbol of companionship and happiness.'),\n",
       " Document(metadata={'source': 'bird-behaviour-doc'}, page_content='Birds are fascinating creatures with a wide range of behaviors and species.'),\n",
       " Document(metadata={'source': 'aquatic-animals-doc'}, page_content='Fish are aquatic animals that are omnivores, meaning they eat a variety of foods.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "os.environ['HF_TOKEN'] = os.getenv('HF_TOKEN')\n",
    "\n",
    "llm = ChatGroq(model =\"Llama3-8b-8192\", groq_api_key = groq_api_key) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15512\\anaconda3\\envs\\genai_venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x1ecb6c9f9d0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## vetor stores are used to store the embeddings og the input documents\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents,embedding=embeddings)\n",
    "vectorstore "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are intelligent and playful creatures, often seen as a symbol of companionship and happiness.'),\n",
       " Document(metadata={'source': 'bird-behaviour-doc'}, page_content='Birds are fascinating creatures with a wide range of behaviors and species.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyality and friendliness.'),\n",
       " Document(metadata={'source': 'aquatic-animals-doc'}, page_content='Fish are aquatic animals that are omnivores, meaning they eat a variety of foods.')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search('cat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs an **asynchronous similarity search** on a `vectorstore` using the query \"fish\". Let's break down what this means:\n",
    "\n",
    "**1. Vectorstore:**\n",
    "\n",
    "* A vectorstore is a specialized database that stores embeddings (vector representations) of your data (e.g., documents, images).\n",
    "* These embeddings capture the semantic meaning of your data, allowing for similarity search.\n",
    "\n",
    "**2. `asimilarity_search()`:**\n",
    "\n",
    "* This method is used to find the most semantically similar items in the vectorstore to a given query.\n",
    "* In this case, the query is \"fish\". The `asimilarity_search()` method will calculate the similarity between the embedding of \"fish\" and the embeddings of all the items stored in the vectorstore.\n",
    "* It will then return a list of the most similar items (e.g., documents, images) based on a similarity score.\n",
    "\n",
    "**3. `await`:**\n",
    "\n",
    "* The `await` keyword indicates that this is an asynchronous operation.\n",
    "* Asynchronous operations allow your program to continue executing other tasks while waiting for the similarity search to complete. This is particularly useful when dealing with potentially long-running operations like database queries.\n",
    "\n",
    "**In simpler terms:**\n",
    "\n",
    "Imagine you have a library of documents about various aquatic creatures. You want to find the documents that are most relevant to \"fish\".  \n",
    "\n",
    "This code does the following:\n",
    "\n",
    "1.  **Converts \"fish\" into an embedding:** This captures the meaning of \"fish\" in a numerical vector format.\n",
    "2.  **Compares the \"fish\" embedding to all other embeddings in the vectorstore:** This identifies documents with similar meanings.\n",
    "3.  **Returns the most similar documents:** You get a list of documents that are most likely related to \"fish\".\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "# Assuming you have a vectorstore called 'my_vectorstore'\n",
    "results = await my_vectorstore.asimilarity_search('fish')\n",
    "\n",
    "# Print the content of the most similar document\n",
    "print(results[0].page_content)\n",
    "```\n",
    "\n",
    "This might output something like:\n",
    "\n",
    "```\n",
    "\"Fish are aquatic vertebrates that have gills for breathing and fins for swimming. They come in a wide variety of shapes and sizes...\" \n",
    "```\n",
    "\n",
    "**Key takeaway:**\n",
    "\n",
    "`await vectorstore.asimilarity_search('fish')` efficiently retrieves the most semantically relevant items related to \"fish\" from a vectorstore using an asynchronous operation. This is a powerful technique for building applications that require semantic search capabilities, such as information retrieval, question answering, and recommendation systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'aquatic-animals-doc'}, page_content='Fish are aquatic animals that are omnivores, meaning they eat a variety of foods.'),\n",
       " Document(metadata={'source': 'bird-behaviour-doc'}, page_content='Birds are fascinating creatures with a wide range of behaviors and species.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are intelligent and playful creatures, often seen as a symbol of companionship and happiness.'),\n",
       " Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyality and friendliness.')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## async query\n",
    "\n",
    "await vectorstore.asimilarity_search('fish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are intelligent and playful creatures, often seen as a symbol of companionship and happiness.'),\n",
       "  1.0024406909942627),\n",
       " (Document(metadata={'source': 'bird-behaviour-doc'}, page_content='Birds are fascinating creatures with a wide range of behaviors and species.'),\n",
       "  1.578450322151184),\n",
       " (Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyality and friendliness.'),\n",
       "  1.5882408618927002),\n",
       " (Document(metadata={'source': 'aquatic-animals-doc'}, page_content='Fish are aquatic animals that are omnivores, meaning they eat a variety of foods.'),\n",
       "  1.6116491556167603)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore.similarity_search_with_score('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are intelligent and playful creatures, often seen as a symbol of companionship and happiness.')],\n",
       " [Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyality and friendliness.')]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Retrivers\n",
    "from typing import List\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "retriver = RunnableLambda(vectorstore.similarity_search).bind(k=1)\n",
    "\n",
    "retriver.batch(['cat','dog'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vecot store implementation with as_retriever\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(metadata={'source': 'mammal-pets-doc'}, page_content='Cats are intelligent and playful creatures, often seen as a symbol of companionship and happiness.')],\n",
       " [Document(metadata={'source': 'mammal-pets-doc'}, page_content='Dogs are great companions, known for their loyality and friendliness.')]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type = 'similarity',\n",
    "    search_kwargs = {'k':1}\n",
    ")\n",
    "\n",
    "retriver.batch(['cat','dog'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the provided context, dogs are great companions, known for their loyalty and friendliness.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "message = \"\"\"\n",
    "Answer this question using the provided context only.\n",
    "\n",
    "{question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([('human', message)])\n",
    "\n",
    "rag_chain = {'context':retriever, \"question\":RunnablePassthrough()}|prompt|llm\n",
    "\n",
    "response = rag_chain.invoke('tell me about dogs')\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
